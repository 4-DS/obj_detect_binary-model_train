{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ff3a0d-0c43-4065-9415-d6984a1b7e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5801a34e-c02b-4eb2-abd3-2ca860d3cc3b",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# specify parameters\n",
    "pipeline_params={\n",
    "}\n",
    "step_params={\n",
    "}\n",
    "substep_params={\n",
    "    \"SEED\"         : 42, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9452dc47-ec58-4ba9-b2e2-a3f6aae3100e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define substep interface\n",
    "from sinara.substep import NotebookSubstep, default_param_values, ENV_NAME, PIPELINE_NAME, ZONE_NAME, STEP_NAME, RUN_ID, ENTITY_NAME, ENTITY_PATH, SUBSTEP_NAME\n",
    "\n",
    "substep = NotebookSubstep(pipeline_params, step_params, substep_params, **default_param_values(\"params/step_params.json\"))\n",
    "\n",
    "substep.interface(\n",
    "    # tmp results from previous step\n",
    "    tmp_inputs = \n",
    "    [\n",
    "        { ENTITY_NAME: \"train_eval_config\" },\n",
    "        { ENTITY_NAME: \"pretrain_weights\" },\n",
    "        { ENTITY_NAME: \"yolox_obj_detector_work_dir\" }\n",
    "    ],    \n",
    "    outputs = \n",
    "    [\n",
    "        { ENTITY_NAME: \"yolox_obj_detector_archive\"} # stored detector files\n",
    "    ]\n",
    ")\n",
    "\n",
    "substep.print_interface_info()\n",
    "\n",
    "substep.exit_in_visualize_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ec490a-1491-4203-a79d-2abd08df4d13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set logging\n",
    "import logging\n",
    "import os.path as osp\n",
    "import os\n",
    "logging.root.setLevel(substep_params.get('loggingLevel', 'INFO'))\n",
    "logging.debug('Debug Output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2bac9a-fa90-4f3d-9789-66da9eddcf7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Initializing for training model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bed10f-9967-4d1d-9e36-6be7c9c7df3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Initializing modules from mmdetection, mmcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0b52e9-2386-48e6-9b76-5bf8869465e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Initializing modules \n",
    "import torch\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import mmcv\n",
    "from mmcv import Config, ConfigDict\n",
    "\n",
    "import mmdet\n",
    "from mmdet.apis import init_random_seed, set_random_seed, train_detector\n",
    "from mmdet.datasets import build_dataset\n",
    "from mmdet.models import build_detector\n",
    "from mmdet.utils import get_root_logger\n",
    "\n",
    "# Checking the version of libraries and checking the availability of the cuda kernel\n",
    "assert torch.cuda.is_available(), f\"Cuda not available\"\n",
    "if torch.cuda.is_available():\n",
    "    device_id = torch.cuda.current_device()\n",
    "    device_name = torch.cuda.get_device_name(device_id)\n",
    "    print(f\"{device_name=}\")\n",
    "    print(f\"{torch.cuda.device_count()=}\")\n",
    "\n",
    "# registry augmenation - DataAsList\n",
    "from mmdet.datasets import PIPELINES\n",
    "try:\n",
    "    @PIPELINES.register_module()\n",
    "    class DataAsList:\n",
    "        def __call__(self, results):\n",
    "            aug_data_dict = {key: [val] for key, val in results.items()}\n",
    "            return aug_data_dict\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de183f28-6c05-428b-812f-6248c83b1b54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining basic variables from the config\n",
    "import json\n",
    "tmp_inputs = substep.tmp_inputs()\n",
    "\n",
    "config_fn = os.path.join(tmp_inputs.train_eval_config, 'config.json')\n",
    "\n",
    "with open(config_fn) as f_id:\n",
    "    CONFIG = json.load(f_id)    \n",
    "mmcv_cfg = Config.fromfile(CONFIG['config_file'])\n",
    "with open(osp.join(mmcv_cfg.work_dir, 'config.json'), 'w') as f_id:\n",
    "    json.dump(CONFIG, f_id, indent=4)    \n",
    "    \n",
    "base_seed = substep_params['SEED']**2\n",
    "deterministic = False\n",
    "\n",
    "# get configurations model from config\n",
    "meta = dict()\n",
    "meta['config'] = mmcv_cfg.pretty_text\n",
    "# set random seeds\n",
    "seed = init_random_seed(base_seed)\n",
    "set_random_seed(seed, deterministic=deterministic)\n",
    "mmcv_cfg['seed'] = seed\n",
    "meta['seed'] = seed\n",
    "meta['exp_name'] = CONFIG['config_file']\n",
    "\n",
    "# init the logger before other steps\n",
    "timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n",
    "log_file = osp.join(mmcv_cfg.work_dir, f'latest.log')\n",
    "logger = get_root_logger(log_file=log_file, log_level=mmcv_cfg.log_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59e75fc-d48b-4448-adbf-ff9025c04989",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Initializing the model based on pretrain weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dc5786-ec05-40f6-830e-70e6f49250a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = build_detector(mmcv_cfg.model)\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1133ad4f-3145-4b9b-b9b9-7b4d0f09d80e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Initializing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08642c01-575f-44f5-9bfb-1a05fc932938",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FIXME - can't understand\n",
    "datasets = [build_dataset(mmcv_cfg.data.train)]\n",
    "\n",
    "if len(mmcv_cfg.workflow) == 2:\n",
    "    val_dataset = copy.deepcopy(mmcv_cfg.data.val)\n",
    "    val_dataset.pipeline = mmcv_cfg.data.train.pipeline\n",
    "    datasets.append(build_dataset(val_dataset))\n",
    "    \n",
    "for i in range(len(datasets)):\n",
    "    try:\n",
    "        datasets[i].update_skip_type_keys\n",
    "    except AttributeError:\n",
    "        datasets[i].update_skip_type_keys = lambda x: x \n",
    "        \n",
    "model.CLASSES = datasets[0].CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae00a2f-6230-4e50-8735-7e4b306b5e88",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Start model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd95162-04e9-4c90-a4c4-fdd96675bc8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " train_detector(\n",
    "    model,\n",
    "    datasets,\n",
    "    mmcv_cfg,\n",
    "    validate=True,\n",
    "    timestamp=timestamp,\n",
    "    meta=meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184db844-5828-4715-bf45-e96690211096",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data preparation after training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832beb9f-ef9f-4ec0-a21a-91903f33b496",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Copying an image from a validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515856d5-2db0-46aa-b9df-32afddf69cff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save one example image from eval_dataset\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil \n",
    "\n",
    "def load_json(json_file):\n",
    "    with open(json_file) as io:\n",
    "        json_data = json.load(io)\n",
    "    return json_data\n",
    "\n",
    "val_coco = load_json(val_dataset.ann_file)\n",
    "assert val_coco\n",
    "select_file = osp.join(val_dataset.img_prefix, val_coco[\"images\"][0][\"file_name\"])\n",
    "assert osp.exists(select_file)\n",
    "\n",
    "shutil.copy(select_file, osp.join(CONFIG['work_dir'], f\"test{Path(select_file).suffix}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b22b1a0-a520-4cfb-908f-c6661af31b31",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Copying a trained model\n",
    "(weights, config, test image) for subsequent transfer to other components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5eb6e1-50cd-4b53-b875-84a2130df072",
   "metadata": {},
   "source": [
    "Since during the training process intermediate weights of the neural network can be created (for example, for epochs 10, 20, 30, etc.)\n",
    "then it doesn't make much sense to copy all the intermediate files to another step in the pipeline.\n",
    "Therefore, we will copy the weights and the necessary configs into a separate directory and we will copy these files to outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469cd9f9-d0b1-4c7b-bb7b-05710b62ff17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy files (last and best model weights and config model to finished dir\n",
    "import shutil \n",
    "import os.path as osp\n",
    "\n",
    "weights_dir = osp.join(mmcv_cfg.work_dir, \"weights\")\n",
    "os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "model_path = mmcv_cfg.work_dir\n",
    "files = [osp.join(model_path, file) for file in os.listdir(model_path)]\n",
    "models_pth  = [file for file in files if '.pth' in file if osp.isfile(file)]\n",
    "best_models = [file for file in models_pth if 'best' in file]\n",
    "latest_models = [file for file in models_pth if 'latest' in file]\n",
    "\n",
    "shutil.copy(osp.join(mmcv_cfg.work_dir, \"config.json\"), osp.join(weights_dir, \"config.json\"))\n",
    "shutil.copy(osp.join(mmcv_cfg.work_dir, f\"test{Path(select_file).suffix}\"), osp.join(weights_dir, f\"test{Path(select_file).suffix}\"))\n",
    "shutil.copy(mmcv_cfg.filename, osp.join(weights_dir, osp.basename(mmcv_cfg.filename)))\n",
    "\n",
    "for fpath in latest_models:\n",
    "    shutil.copy(fpath, fpath.replace(model_path, weights_dir))\n",
    "for fpath in best_models:\n",
    "    shutil.copy(fpath, fpath.replace(model_path, weights_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655cb9dd-6115-4f22-bb77-5f9bc39b1a69",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Preparing the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6e415c-c2f8-429e-aa38-d1d9d1e3d457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delete information in configs about use tmp entities\n",
    "mmcv_cfg = Config.fromfile(osp.join(weights_dir, \"last_cfg.py\"))\n",
    "mmcv_cfg.load_from = \"\"\n",
    "mmcv_cfg.train_dataset.ann_file = \"\"\n",
    "mmcv_cfg.test_dataset.ann_file = \"\"\n",
    "mmcv_cfg.data.train.ann_file = \"\"\n",
    "mmcv_cfg.data.val.ann_file = \"\"\n",
    "mmcv_cfg.data.test.ann_file = \"\"\n",
    "mmcv_cfg.work_dir = \"\" \n",
    "config_file = osp.join(mmcv_cfg.work_dir, \"last_cfg.py\")\n",
    "mmcv_cfg.dump(file=osp.join(weights_dir, \"last_cfg.py\"))\n",
    "\n",
    "with open(osp.join(weights_dir, \"config.json\")) as f_id:\n",
    "    temp_CONFIG = json.load(f_id)    \n",
    "temp_CONFIG.pop(\"eval_datasets\") if \"eval_datasets\" in temp_CONFIG else \"\"\n",
    "temp_CONFIG.pop(\"train_datasets\") if \"train_datasets\" in temp_CONFIG else \"\"\n",
    "temp_CONFIG.pop(\"train_output_file\") if \"train_output_file\" in temp_CONFIG else \"\"\n",
    "temp_CONFIG.pop(\"eval_output_file\") if \"eval_output_file\" in temp_CONFIG else \"\"\n",
    "temp_CONFIG.pop(\"work_dir\") if \"work_dir\" in temp_CONFIG else \"\"\n",
    "temp_CONFIG[\"config_file\"] = osp.basename(temp_CONFIG[\"config_file\"])\n",
    "\n",
    "with open(osp.join(weights_dir, \"config.json\"), 'w') as f_id:\n",
    "    json.dump(temp_CONFIG, f_id, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa64369-efb6-45fe-8c2d-689ffadaa97a",
   "metadata": {},
   "source": [
    "### Send training model to outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2275ca57-1af4-461b-abb0-c6b5d2dcb145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# saves tmp_entities (weights_dir)  to outputs (yolox_obj_detector) of step model_pack\n",
    "from sinara.store import SinaraStore\n",
    "outputs = substep.outputs()\n",
    "SinaraStore.archive_tmp_files_to_store(tmp_dir=weights_dir, store_path=outputs.yolox_obj_detector_archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16a2604-bad8-40a4-8c39-b4cabb8ff56c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
